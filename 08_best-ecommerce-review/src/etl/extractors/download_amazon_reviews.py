"""
Amazon Review Dataset ë‹¤ìš´ë¡œë“œ
Julian McAuley êµìˆ˜ì˜ ê³µê°œ ë°ì´í„°ì…‹

ë°ì´í„°ì…‹ ì •ë³´:
- ì¶œì²˜: UCSD (University of California, San Diego)
- ê·œëª¨: ìˆ˜ë°±ë§Œ ê±´ (ì¹´í…Œê³ ë¦¬ë³„)
- ì–¸ì–´: ì˜ì–´
- ê¸°ê°„: 1996-2018
- ë¼ì´ì„ ìŠ¤: í•™ìˆ /ì—°êµ¬ìš© ê³µê°œ

ì¹´í…Œê³ ë¦¬:
- Electronics (ì „ìì œí’ˆ)
- Clothing (ì˜ë¥˜)
- Books (ë„ì„œ)
- Home & Kitchen (ê°€ì •ìš©í’ˆ)
- ë“± 30ê°œ ì´ìƒ
"""

import requests
import gzip
import json
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import argparse


# Amazon Review ë°ì´í„°ì…‹ URL
AMAZON_DATASETS = {
    'electronics': {
        'name': 'Electronics',
        'url': 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Electronics.jsonl.gz',
        'size': 'ì•½ 700ë§Œ ê±´',
        'sample_url': 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Electronics_sample.jsonl.gz',
    },
    'clothing': {
        'name': 'Clothing, Shoes and Jewelry',
        'url': 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Clothing_Shoes_and_Jewelry.jsonl.gz',
        'size': 'ì•½ 1,000ë§Œ ê±´',
        'sample_url': 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Clothing_sample.jsonl.gz',
    },
    'home': {
        'name': 'Home and Kitchen',
        'url': 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Home_and_Kitchen.jsonl.gz',
        'size': 'ì•½ 500ë§Œ ê±´',
        'sample_url': 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Home_sample.jsonl.gz',
    },
    'beauty': {
        'name': 'Beauty and Personal Care',
        'url': 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Beauty_and_Personal_Care.jsonl.gz',
        'size': 'ì•½ 700ë§Œ ê±´',
        'sample_url': 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Beauty_sample.jsonl.gz',
    },
}


def print_amazon_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ Amazon ë°ì´í„°ì…‹ ì¶œë ¥"""
    print("\nğŸ“¦ Amazon Review ë°ì´í„°ì…‹ ëª©ë¡:\n")
    
    for key, info in AMAZON_DATASETS.items():
        print(f"ğŸ”¹ {info['name']}")
        print(f"   í‚¤: {key}")
        print(f"   ê·œëª¨: {info['size']}")
        print()
    
    print("ğŸ’¡ ìƒ˜í”Œ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤!")


def download_file(url, output_path, chunk_size=8192):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì§„í–‰ë¥  í‘œì‹œ)"""
    
    print(f"\nğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {url}")
    
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'wb') as f, tqdm(
            total=total_size,
            unit='B',
            unit_scale=True,
            desc=Path(output_path).name
        ) as pbar:
            for chunk in response.iter_content(chunk_size=chunk_size):
                if chunk:
                    f.write(chunk)
                    pbar.update(len(chunk))
        
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path}")
        return output_path
        
    except requests.exceptions.HTTPError as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (HTTP ì˜¤ë¥˜): {e}")
        print("âš ï¸  URLì´ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³µì‹ ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
        print("   https://cseweb.ucsd.edu/~jmcauley/datasets.html")
        return None
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def extract_gz(gz_path, output_jsonl=None):
    """GZIP ì••ì¶• í•´ì œ"""
    
    if not output_jsonl:
        output_jsonl = str(gz_path).replace('.gz', '')
    
    print(f"\nğŸ“¦ ì••ì¶• í•´ì œ ì¤‘: {gz_path}")
    
    try:
        with gzip.open(gz_path, 'rb') as f_in:
            with open(output_jsonl, 'wb') as f_out:
                f_out.write(f_in.read())
        
        print(f"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ: {output_jsonl}")
        return output_jsonl
        
    except Exception as e:
        print(f"âŒ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
        return None


def convert_amazon_to_standard(jsonl_path, output_csv, max_rows=None):
    """
    Amazon JSONLì„ í”„ë¡œì íŠ¸ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Amazon ë°ì´í„° êµ¬ì¡°:
    {
        "rating": 5.0,
        "text": "Great product!",
        "timestamp": 1234567890,
        "asin": "B001...",
        ...
    }
    """
    
    print(f"\nğŸ”„ ë°ì´í„° ë³€í™˜ ì¤‘: {jsonl_path}")
    
    reviews = []
    
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if max_rows and i >= max_rows:
                    break
                
                try:
                    data = json.loads(line)
                    reviews.append({
                        'review_id': f"AMZN_{data.get('asin', 'unknown')}_{i:08d}",
                        'source': 'amazon',
                        'created_date': pd.to_datetime(data.get('timestamp', 0), unit='s').date(),
                        'review_content': data.get('text', ''),
                        'rating': data.get('rating', 0),
                        'author_masked': 'ìµëª…',
                        'product_category': 'Amazon',
                    })
                except:
                    continue
                
                if i % 100000 == 0 and i > 0:
                    print(f"   ì²˜ë¦¬ ì¤‘: {i:,}ê°œ...")
        
        print(f"âœ… ì½ê¸° ì™„ë£Œ: {len(reviews):,}ê°œ")
        
        # DataFrame ë³€í™˜
        df = pd.DataFrame(reviews)
        
        # ë°ì´í„° ì •ì œ
        df = df.dropna(subset=['review_content'])
        df = df[df['review_content'].str.len() >= 10]
        df = df[df['rating'] > 0]
        
        # ì €ì¥
        Path(output_csv).parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(output_csv, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ë³€í™˜ ì™„ë£Œ!")
        print(f"   - ì›ë³¸: {len(reviews):,}ê°œ")
        print(f"   - ì •ì œ í›„: {len(df):,}ê°œ")
        print(f"   - ì €ì¥: {output_csv}")
        
        # ìƒ˜í”Œ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„°:")
        print(df.head(3)[['created_date', 'rating', 'review_content']])
        
        # í†µê³„
        print(f"\nğŸ“Š í†µê³„:")
        print(f"   í‰ê·  í‰ì : {df['rating'].mean():.2f}")
        print(f"   í‰ê·  ê¸¸ì´: {df['review_content'].str.len().mean():.1f}ì")
        print(f"   ë‚ ì§œ ë²”ìœ„: {df['created_date'].min()} ~ {df['created_date'].max()}")
        
        return output_csv
        
    except Exception as e:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    parser = argparse.ArgumentParser(description='Amazon Review ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('--category', type=str, choices=list(AMAZON_DATASETS.keys()) + ['list'],
                       help='ì¹´í…Œê³ ë¦¬ ì„ íƒ (list: ëª©ë¡ ì¶œë ¥)')
    parser.add_argument('--sample', action='store_true', help='ìƒ˜í”Œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ë¹ ë¦„)')
    parser.add_argument('--max-rows', type=int, help='ìµœëŒ€ ë³€í™˜ í–‰ ìˆ˜ (ë©”ëª¨ë¦¬ ì ˆì•½)')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ğŸŒ Amazon Review ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
    print("=" * 70)
    
    if not args.category or args.category == 'list':
        print_amazon_datasets()
        print("\nì‚¬ìš© ì˜ˆì‹œ:")
        print("  python download_amazon_reviews.py --category electronics --sample")
        print("  python download_amazon_reviews.py --category clothing --max-rows 50000")
        return
    
    dataset = AMAZON_DATASETS[args.category]
    
    print(f"\nğŸ“¦ ì„ íƒëœ ë°ì´í„°ì…‹: {dataset['name']}")
    print(f"ê·œëª¨: {dataset['size']}")
    
    # URL ì„ íƒ (ìƒ˜í”Œ ë˜ëŠ” ì „ì²´)
    url = dataset['sample_url'] if args.sample else dataset['url']
    filename = f"{args.category}_{'sample' if args.sample else 'full'}.jsonl.gz"
    
    # ë‹¤ìš´ë¡œë“œ
    output_dir = 'data/bronze/amazon'
    gz_path = Path(output_dir) / filename
    
    if not gz_path.exists():
        downloaded = download_file(url, gz_path)
        if not downloaded:
            return
    else:
        print(f"\nâœ… íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {gz_path}")
    
    # ì••ì¶• í•´ì œ
    jsonl_path = str(gz_path).replace('.gz', '')
    
    if not Path(jsonl_path).exists():
        extracted = extract_gz(gz_path, jsonl_path)
        if not extracted:
            return
    else:
        print(f"\nâœ… ì••ì¶• í•´ì œëœ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {jsonl_path}")
    
    # CSV ë³€í™˜
    output_csv = f"{output_dir}/reviews_{args.category}_{'sample' if args.sample else 'full'}.csv"
    
    convert_amazon_to_standard(jsonl_path, output_csv, max_rows=args.max_rows)
    
    print("\n" + "=" * 70)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 70)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  python src/etl/extractors/validate_smartstore_csv.py")
    print(f"  â†’ ê²½ë¡œ: {output_csv}")


if __name__ == "__main__":
    main()
